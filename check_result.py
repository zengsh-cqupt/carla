from vae_unit import load_mnist_data, plot_image_rows, load_carla_data
import matplotlib.pyplot as plt
from keras.losses import mse, binary_crossentropy
from keras import backend as K
from keras import layers
import keras
from keras.models import Model, load_model
import numpy as np
from config import IMG_SIZE, mode, epochs, latent_dim, beta, scale, scale_r, lr, use_pretrained, filepath, batch_size
from keras.callbacks import ModelCheckpoint
import vae_unit as vae_util
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import (KNeighborsClassifier,
                               NeighborhoodComponentsAnalysis)
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from mpl_toolkits.mplot3d import Axes3D

latent_dim = 128
def create_vae(latent_dim, return_kl_loss_op=False):
    encoder = vae_util.create_encoder(latent_dim)
    decoder = vae_util.create_decoder(latent_dim)
    sampler = vae_util.create_sampler()

    x = layers.Input(shape=IMG_SIZE, name='image')
    t_mean, t_log_var = encoder(x)
    t = sampler([t_mean, t_log_var])
    t_decoded = decoder(t)

    model = Model(x, t_decoded, name='vae')

    if return_kl_loss_op:
        kl_loss = -0.5 * K.sum(1 + t_log_var - K.square(t_mean) - K.exp(t_log_var), axis=-1)
        return model, kl_loss
    else:
        return model


def vae_loss(x, t_decoded):
    return scale * K.mean(scale_r * reconstruction_loss(x, t_decoded) + beta * vae_kl_loss)  # adjust losss scale


def reconstruction_loss(x, t_decoded):
    return K.sum(K.square(x - t_decoded), axis=(1, 2, 3))  # average over images


if mode[:5] == "carla":
    x_train, x_test = load_carla_data(num=2000, normalize=True)
else:
    (x_train, _), (x_test, _) = load_mnist_data(normalize=True)

vae, vae_kl_loss = create_vae(latent_dim, return_kl_loss_op=True)
# filepath = "/home/gu/project/ppo/ppo_carla/models/carla/high_ld_512_beta_1_r_1_lr_0.0001.hdf5"
filepath = "/home/gu/project/ppo/ppo_carla/models/carla_model/large_high_ld_128_beta_2_r_1_lr_0.0001_bc_128.hdf5"
print(filepath)
vae.load_weights(filepath)


def encode(model, images):
    return model.get_layer('encoder').predict(images)[0]


def decode(model, codes):
    return model.get_layer('decoder').predict(codes)


def encode_decode(model, images):
    return decode(model, encode(model, images))


selected_idx = np.random.choice(range(x_test.shape[0]), 10, replace=False)
selected = x_test[selected_idx]
selected_dec_vae = encode_decode(vae, selected)
plot_image_rows([selected, selected_dec_vae],
                ['Original images',
                 'Images generated by plain VAE'])
plt.show()


selected_idx = np.random.choice(range(x_test.shape[0]), 1000, replace=False)
selected = x_test[selected_idx]
latent_space = encode(vae, selected)


# Reduce dimension to 2 with PCA
pca = make_pipeline(StandardScaler(),
                    PCA(n_components=3))


# Make a list of the methods to be compared
dim_reduction_methods = [('PCA', pca)]

# plt.figure()
for i, (name, model) in enumerate(dim_reduction_methods):
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    # plt.subplot(1, 3, i + 1, aspect=1)

    # Fit the method's model
    model.fit(latent_space)

    # Embed the data set in 2 dimensions using the fitted model
    X_embedded = model.transform(latent_space)

    # Plot the projected points and show the evaluation score
    # plt.scatter(X_embedded[:, 0], X_embedded[:, 1])
    ax.scatter(X_embedded[:, 0], X_embedded[:, 1], X_embedded[:, 2], cmap='Greens')

plt.show()